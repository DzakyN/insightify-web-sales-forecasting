{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM1lO1dh82G9+m1Vam+0fCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DzakyN/insightify-web-sales-forecasting/blob/main/Model_TIKEXPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forecasting metode agregasi model LSTM"
      ],
      "metadata": {
        "id": "PfLV8IEtP5ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Attention, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "import holidays\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "# 1 Seed\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "# 2ï¸ Load data\n",
        "file_path = r\"C:\\Users\\mdzak\\Downloads\\Data_Final_Setelah_Hapus_Store_8.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "# 3ï¸ Group by date\n",
        "df_total = df.groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index().sort_values(\"Date\")\n",
        "\n",
        "# 4ï¸ Baseline moving average\n",
        "window_baseline = 154\n",
        "df_total[\"baseline_ma30\"] = df_total[\"Weekly_Sales\"].rolling(window=window_baseline, min_periods=1).mean()\n",
        "\n",
        "\n",
        "# 5ï¸ Tambahkan fitur musiman & lag\n",
        "df_total[\"dayofmonth\"] = df_total[\"Date\"].dt.day\n",
        "df_total[\"weekofyear\"] = df_total[\"Date\"].dt.isocalendar().week\n",
        "df_total[\"is_month_start\"] = df_total[\"Date\"].dt.is_month_start.astype(int)\n",
        "df_total[\"sin_week\"] = np.sin(2 * np.pi * df_total[\"weekofyear\"] / 52)\n",
        "df_total[\"cos_week\"] = np.cos(2 * np.pi * df_total[\"weekofyear\"] / 52)\n",
        "df_total[\"lag_1\"] = df_total[\"Weekly_Sales\"].shift(1)\n",
        "df_total[\"lag_52\"] = df_total[\"Weekly_Sales\"].shift(52)\n",
        "\n",
        "\n",
        "# 6ï¸ Tambahkan exogenous features\n",
        "us_holidays = holidays.US()\n",
        "df_total[\"is_holiday_us\"] = df_total[\"Date\"].apply(lambda x: 1 if x in us_holidays else 0)\n",
        "df_total[\"is_weekend\"] = df_total[\"Date\"].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)\n",
        "df_total[\"month\"] = df_total[\"Date\"].dt.month\n",
        "df_total[\"quarter\"] = df_total[\"Date\"].dt.quarter\n",
        "df_total[\"rolling_mean_7\"] = df_total[\"Weekly_Sales\"].rolling(window=7, min_periods=1).mean()\n",
        "df_total[\"rolling_std_7\"] = df_total[\"Weekly_Sales\"].rolling(window=7, min_periods=1).std()\n",
        "\n",
        "\n",
        "# Drop NA\n",
        "df_total.dropna(inplace=True)\n",
        "df_total.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "# 7ï¸ Target residual\n",
        "df_total[\"residual_target\"] = df_total[\"Weekly_Sales\"] - df_total[\"baseline_ma30\"]\n",
        "\n",
        "\n",
        "# 8ï¸ Features\n",
        "features = [\"Weekly_Sales\", \"lag_1\", \"lag_52\", \"dayofmonth\", \"is_month_start\",\n",
        "            \"sin_week\", \"cos_week\", \"is_holiday_us\", \"is_weekend\",\n",
        "            \"month\", \"quarter\", \"rolling_mean_7\", \"rolling_std_7\"]\n",
        "\n",
        "\n",
        "# Sliding window\n",
        "window_size = 84\n",
        "forecast_horizon = 90\n",
        "X, y = [], []\n",
        "for i in range(window_size, len(df_total) - forecast_horizon + 1):\n",
        "    X_window = df_total[features].iloc[i-window_size:i].values\n",
        "    y_horizon = df_total[\"residual_target\"].iloc[i:i+forecast_horizon].values\n",
        "    X.append(X_window)\n",
        "    y.append(y_horizon)\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# Split\n",
        "n = len(X)\n",
        "cutoff_train = int(n * 0.67)\n",
        "cutoff_valid = int(n * 0.75)\n",
        "X_train, y_train = X[:cutoff_train], y[:cutoff_train]\n",
        "X_valid, y_valid = X[cutoff_train:cutoff_valid], y[cutoff_train:cutoff_valid]\n",
        "X_test, y_test = X[cutoff_valid:], y[cutoff_valid:]\n",
        "\n",
        "\n",
        "# Scaling\n",
        "y_scaler = MinMaxScaler()\n",
        "y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "y_valid_scaled = y_scaler.transform(y_valid)\n",
        "y_test_scaled = y_scaler.transform(y_test)\n",
        "\n",
        "\n",
        "x_scaler = MinMaxScaler()\n",
        "X_train_scaled = x_scaler.fit_transform(X_train.reshape(-1, len(features))).reshape((-1, window_size, len(features)))\n",
        "X_valid_scaled = x_scaler.transform(X_valid.reshape(-1, len(features))).reshape((-1, window_size, len(features)))\n",
        "X_test_scaled = x_scaler.transform(X_test.reshape(-1, len(features))).reshape((-1, window_size, len(features)))\n",
        "\n",
        "\n",
        "# Model stacked LSTM + Attention\n",
        "def create_model(input_shape, units=128, dropout=0.2, lr=0.001):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(units, return_sequences=True, dropout=dropout)(inputs)\n",
        "    x = LSTM(units, return_sequences=True, dropout=dropout)(x)\n",
        "    attn = Attention()([x, x])\n",
        "    concat = Concatenate()([x, attn])\n",
        "    x = Dense(256, activation='relu')(concat[:, -1, :])\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(forecast_horizon)(x)\n",
        "    model = Model(inputs, out)\n",
        "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"units\": [64, 128],\n",
        "    \"dropout\": [0.1, 0.2],\n",
        "    \"lr\": [0.001, 0.0005],\n",
        "    \"batch_size\": [16, 32]\n",
        "}\n",
        "param_combinations = list(product(param_grid[\"units\"], param_grid[\"dropout\"],\n",
        "                                  param_grid[\"lr\"], param_grid[\"batch_size\"]))\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "best_r2 = -np.inf\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "\n",
        "for units, dropout, lr, batch_size in param_combinations:\n",
        "    print(f\"ğŸ” Training: units={units}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "    model = create_model((window_size, len(features)), units=units, dropout=dropout, lr=lr)\n",
        "    es = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    model.fit(X_train_scaled, y_train_scaled,\n",
        "               validation_data=(X_valid_scaled, y_valid_scaled),\n",
        "               epochs=50, batch_size=batch_size, verbose=0, callbacks=[es])\n",
        "    y_valid_pred_scaled = model.predict(X_valid_scaled, verbose=0)\n",
        "    y_valid_pred = y_scaler.inverse_transform(y_valid_pred_scaled)\n",
        "    y_valid_real = y_scaler.inverse_transform(y_valid_scaled)\n",
        "    baseline_valid = df_total[\"baseline_ma30\"].iloc[cutoff_train+window_size:cutoff_valid+window_size].values.reshape(-1,1).repeat(forecast_horizon, axis=1)\n",
        "    y_valid_pred_final = y_valid_pred + baseline_valid\n",
        "    y_valid_real_final = y_valid_real + baseline_valid\n",
        "    mape = mean_absolute_percentage_error(y_valid_real_final, y_valid_pred_final) * 100\n",
        "    mae = mean_absolute_error(y_valid_real_final, y_valid_pred_final)\n",
        "    rmse = np.sqrt(mean_squared_error(y_valid_real_final, y_valid_pred_final))\n",
        "    r2 = r2_score(y_valid_real_final.flatten(), y_valid_pred_final.flatten())\n",
        "    results.append({\n",
        "        \"units\": units,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAPE\": round(mape, 2),\n",
        "        \"MAE\": round(mae, 2),\n",
        "        \"RMSE\": round(rmse, 2),\n",
        "        \"R2\": round(r2, 4)\n",
        "    })\n",
        "    print(f\"â¡ï¸ Valid Metrics: MAPE={mape:.2f}%, MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
        "    if r2 > best_r2:\n",
        "        best_r2 = r2\n",
        "        best_model = model\n",
        "        best_params = (units, dropout, lr, batch_size)\n",
        "\n",
        "\n",
        "# Log semua hasil\n",
        "print(\"\\nğŸ“ All Hyperparameter Combinations and Validation Metrics:\")\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Best Hyperparameters (RÂ² Based):\")\n",
        "print(f\"Units: {best_params[0]}, Dropout: {best_params[1]}, LR: {best_params[2]}, Batch: {best_params[3]}\")\n",
        "\n",
        "\n",
        "#Final Evaluation on Test\n",
        "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
        "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled)\n",
        "y_test_real = y_scaler.inverse_transform(y_test_scaled)\n",
        "baseline_test = df_total[\"baseline_ma30\"].iloc[-len(y_test):].values.reshape(-1, 1).repeat(forecast_horizon, axis=1)\n",
        "y_test_pred_final = y_test_pred + baseline_test\n",
        "y_test_real_final = y_test_real + baseline_test\n",
        "\n",
        "\n",
        "test_mape = mean_absolute_percentage_error(y_test_real_final, y_test_pred_final) * 100\n",
        "test_mae = mean_absolute_error(y_test_real_final, y_test_pred_final)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test_real_final, y_test_pred_final))\n",
        "test_r2 = r2_score(y_test_real_final.flatten(), y_test_pred_final.flatten())\n",
        "\n",
        "\n",
        "print(\"\\nğŸ¯ Final Test Metrics (Best Model - RÂ² Based):\")\n",
        "print(\"âœ… MAPE:\", round(test_mape, 2), \"%\")\n",
        "print(\"âœ… MAE :\", round(test_mae, 2))\n",
        "print(\"âœ… RMSE:\", round(test_rmse, 2))\n",
        "print(\"âœ… R2  :\", round(test_r2, 4))\n",
        "\n",
        "\n",
        "#Rolling Forecast ke depan 30 hari\n",
        "last_window_real = df_total[features].values[-window_size:]\n",
        "last_window_scaled = x_scaler.transform(last_window_real).reshape(1, window_size, len(features))\n",
        "forecast_scaled = best_model.predict(last_window_scaled)[0]\n",
        "forecast_residual = y_scaler.inverse_transform(forecast_scaled.reshape(1, -1)).flatten()\n",
        "baseline_future = df_total[\"baseline_ma30\"].iloc[-1]\n",
        "forecast_final = forecast_residual + baseline_future\n",
        "\n",
        "\n",
        "# Save & visualisasi\n",
        "last_date = df_total[\"Date\"].max()\n",
        "future_dates = [last_date + timedelta(days=i) for i in range(1, forecast_horizon + 1)]\n",
        "df_forecast = pd.DataFrame({\"Date\": future_dates, \"Forecast\": forecast_final})\n",
        "df_actual = df_total[[\"Date\", \"Weekly_Sales\"]].rename(columns={\"Weekly_Sales\": \"Actual\"})\n",
        "df_final = pd.concat([df_actual, df_forecast.rename(columns={\"Forecast\": \"Actual\"})], ignore_index=True)\n",
        "df_final.to_csv(\"Total_Sales_LSTM_Hybrid_USA_Forecast.csv\", index=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_actual[\"Date\"], df_actual[\"Actual\"], label=\"Actual\")\n",
        "plt.plot(df_forecast[\"Date\"], df_forecast[\"Forecast\"], label=\"Forecast\", linestyle=\"--\")\n",
        "plt.axvline(x=last_date, color='red', linestyle=':', label=\"Forecast Start\")\n",
        "plt.title(f\"Hybrid LSTM+Attention 30D Forecast (MAPE: {test_mape:.2f}%)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"ğŸ‰ Final hybrid residual forecast & hyperparameter tuning selesai!\")\n"
      ],
      "metadata": {
        "id": "MtHOyj0Upoap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forecasting metode Granulalitas model Prophet (105 Kombinasi)"
      ],
      "metadata": {
        "id": "sqW7N47CP9-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv(r\"C:\\Semester 6\\Setsuna\\Data_Aktual_Tanpa_Forecast.csv\")\n",
        "\n",
        "df['ds'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df = df.rename(columns={'Daily_Sales': 'y'})\n",
        "\n",
        "df = df[['ds', 'Store', 'Dept', 'y']].sort_values('ds')\n",
        "\n",
        "stores = df['Store'].unique()\n",
        "depts = df['Dept'].unique()\n",
        "\n",
        "\n",
        "# Grid search parameter\n",
        "param_grid = {\n",
        "    'changepoint_prior_scale': [0.001, 0.01, 0.1,0.2, 0.5, 1.0,10.0],\n",
        "    'seasonality_prior_scale': [0.001,0.01, 0.1, 1.0,0.2,0.5, 10.0]\n",
        "}\n",
        "\n",
        "# Hasil akhir\n",
        "evaluasi_list = []\n",
        "forecast_list = []\n",
        "\n",
        "\n",
        "for store in stores:\n",
        "    for dept in depts:\n",
        "        subset = df[(df['Store'] == store) & (df['Dept'] == dept)].copy()\n",
        "        if len(subset) < 100:\n",
        "            continue\n",
        "        # Split train-valid-test\n",
        "        n = len(subset)\n",
        "        cutoff_train = int(n * 0.7)\n",
        "        cutoff_valid = int(n * 0.87)\n",
        "\n",
        "\n",
        "        df_train = subset.iloc[:cutoff_train].copy()\n",
        "        df_valid = subset.iloc[cutoff_train:cutoff_valid].copy()\n",
        "        df_test = subset.iloc[cutoff_valid:].copy()\n",
        "\n",
        "\n",
        "        # Tuning\n",
        "        tuning_results = []\n",
        "        for cps in param_grid['changepoint_prior_scale']:\n",
        "            for sps in param_grid['seasonality_prior_scale']:\n",
        "                try:\n",
        "                    model = Prophet(\n",
        "                        daily_seasonality=True,\n",
        "                        weekly_seasonality=True,\n",
        "                        yearly_seasonality=True,\n",
        "                        changepoint_prior_scale=cps,\n",
        "                        seasonality_prior_scale=sps\n",
        "                    )\n",
        "                    model.fit(df_train)\n",
        "                    forecast_valid = model.predict(df_valid[['ds']])\n",
        "                    y_true = df_valid['y'].values\n",
        "                    y_pred = forecast_valid['yhat'].values\n",
        "                    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-5))) * 100\n",
        "                    tuning_results.append({\n",
        "                        'cps': cps,\n",
        "                        'sps': sps,\n",
        "                        'mape': mape\n",
        "                    })\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not tuning_results:\n",
        "            continue\n",
        "\n",
        "        best_param = sorted(tuning_results, key=lambda x: x['mape'])[0]\n",
        "        best_cps = best_param['cps']\n",
        "        best_sps = best_param['sps']\n",
        "\n",
        "\n",
        "        # Rolling Forecast pada Test Set\n",
        "        rolling_train = pd.concat([df_train, df_valid])\n",
        "        y_true_all = []\n",
        "        y_pred_all = []\n",
        "\n",
        "        for i in range(len(df_test)):\n",
        "            model = Prophet(\n",
        "                daily_seasonality=True,\n",
        "                weekly_seasonality=True,\n",
        "                yearly_seasonality=True,\n",
        "                changepoint_prior_scale=best_cps,\n",
        "                seasonality_prior_scale=best_sps\n",
        "            )\n",
        "            model.fit(rolling_train)\n",
        "            future = df_test[['ds']].iloc[i:i+1]\n",
        "            forecast = model.predict(future)\n",
        "\n",
        "\n",
        "            y_true_all.append(df_test['y'].iloc[i])\n",
        "            y_pred_all.append(forecast['yhat'].values[0])\n",
        "            rolling_train = pd.concat([rolling_train, df_test.iloc[i:i+1]], ignore_index=True)\n",
        "\n",
        "        # Evaluasi\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
        "        mae = mean_absolute_error(y_true_all, y_pred_all)\n",
        "        mape = np.mean(np.abs((np.array(y_true_all) - np.array(y_pred_all)) / np.maximum(np.abs(y_true_all), 1e-5))) * 100\n",
        "\n",
        "\n",
        "        evaluasi_list.append({\n",
        "            'Store': store,\n",
        "            'Dept': dept,\n",
        "            'Best CPS': best_cps,\n",
        "            'Best SPS': best_sps,\n",
        "            'RMSE': round(rmse, 3),\n",
        "            'MAE': round(mae, 3),\n",
        "            'MAPE': round(mape, 6)\n",
        "        })\n",
        "\n",
        "\n",
        "        # atur forecast\n",
        "        full_df = pd.concat([df_train, df_valid, df_test])\n",
        "        model_final = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=True,\n",
        "            changepoint_prior_scale=best_cps,\n",
        "            seasonality_prior_scale=best_sps\n",
        "        )\n",
        "        model_final.fit(full_df)\n",
        "        future = model_final.make_future_dataframe(periods=90)\n",
        "        forecast = model_final.predict(future)\n",
        "        last_date = full_df['ds'].max()\n",
        "        forecast_future = forecast[forecast['ds'] > last_date].copy()\n",
        "        forecast_future['Store'] = store\n",
        "        forecast_future['Dept'] = dept\n",
        "        forecast_list.append(forecast_future[['ds', 'Store', 'Dept', 'yhat']])\n",
        "\n",
        "\n",
        "# Simpan Evaluasi dan Forecast\n",
        "df_evaluasi = pd.DataFrame(evaluasi_list)\n",
        "df_evaluasi.to_csv(\"evaluasi_model2.csv\", index=False)\n",
        "\n",
        "\n",
        "df_forecast_all = pd.concat(forecast_list, ignore_index=True)\n",
        "df_forecast_all = df_forecast_all.rename(columns={'ds': 'Date', 'yhat': 'Daily_Sales'})\n",
        "df_forecast_all.to_csv(\"Forecast Final.csv\", index=False)\n",
        "\n",
        "\n",
        "print(\"âœ… Selesai: evaluasi_model.csv & forecast_90hari.csv berhasil dibuat.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kJ3b6DnJpc7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentimen Analisis"
      ],
      "metadata": {
        "id": "L9rXDcaWZC_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk==3.8.1 -q\n",
        "!pip install -U imbalanced-learn -q\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree('/content/nltk_data', ignore_errors=True)\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import joblib  # untuk menyimpan model/vectorizer\n",
        "\n",
        "==\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv(\"/content/Data Sentimen.csv\")\n",
        "\n",
        "# 2. Stopwords default dari NLTK (Bahasa Indonesia)\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# 3. Kata-kata penting yang TIDAK BOLEH DIHAPUS karena punya makna sentimen/konteks\n",
        "kata_penting = {\n",
        "    # Penyangkalan / penekanan\n",
        "    'tidak', 'kurang', 'belum', 'hanya', 'seharusnya',\n",
        "\n",
        "    # Kata-kata temporal / kecepatan / kualitas layanan\n",
        "    'lama', 'cepat', 'lambat', 'terlambat', 'lelet',\n",
        "\n",
        "    # Kata netral / ambiguitas\n",
        "    'biasa', 'lumayan', 'cukup', 'standar','saja'\n",
        "\n",
        "    # Kata negatif umum\n",
        "    'buruk', 'jelek', 'rusak', 'cacat', 'telat',\n",
        "\n",
        "    # Kata positif umum\n",
        "    'bagus', 'baik', 'puas', 'mantap', 'ramah',\n",
        "\n",
        "    # Kata ekspresi\n",
        "    'wah', 'wow', 'parah', 'sumpah', 'love', 'suka', 'senang',\n",
        "\n",
        "    # Kata soal pelayanan / barang\n",
        "    'kasir', 'produk', 'barang', 'layanan', 'pengiriman', 'kurir'\n",
        "}\n",
        "\n",
        "# 4. Hapus kata penting dari stopwords agar tidak terhapus saat filtering\n",
        "stop_words -= kata_penting\n",
        "\n",
        "# 5. Fungsi preprocessing\n",
        "def preprocess_nltk(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)               # hilangkan huruf berulang, misal baaagus â†’ bagus\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)                # hilangkan karakter non-alfabet\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "# 6. Terapkan preprocessing\n",
        "df['clean_text'] = df['review_text'].astype(str).apply(preprocess_nltk)\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=7000,\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "X = tfidf.fit_transform(df['clean_text'])\n",
        "y = df['label_sentimen']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nLaporan klasifikasi:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "joblib.dump(model, 'model_rf.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uoyy_ze_SKpT",
        "outputId": "4e07ff3f-3305-4434-f3a8-35c34624771c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.9325581395348838\n",
            "\n",
            "Laporan klasifikasi:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.90      0.91      0.91       956\n",
            "      netral       0.98      0.93      0.96       722\n",
            "     positif       0.93      0.95      0.94      1332\n",
            "\n",
            "    accuracy                           0.93      3010\n",
            "   macro avg       0.94      0.93      0.93      3010\n",
            "weighted avg       0.93      0.93      0.93      3010\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ğŸ¤ UJI INPUT MANUAL\n",
        "# ================================\n",
        "\n",
        "def prediksi_komentar(text_input):\n",
        "    # Preprocessing\n",
        "    clean = preprocess_nltk(text_input)\n",
        "\n",
        "    # TF-IDF transform (jangan .fit_transform lagi, cukup .transform!)\n",
        "    vectorized = tfidf.transform([clean])\n",
        "\n",
        "    # Prediksi\n",
        "    hasil = model.predict(vectorized)[0]\n",
        "\n",
        "    print(f\"\\nğŸ—£ï¸ Input: {text_input}\")\n",
        "    print(f\"ğŸ§¹ Cleaned: {clean}\")\n",
        "    print(f\"ğŸ“Š Prediksi Sentimen: {hasil}\")\n",
        "\n",
        "# Contoh uji coba:\n",
        "prediksi_komentar(\"pengiriman sangat cepat dan pelayanan ramah sekali\")\n",
        "prediksi_komentar(\"barang rusak, tidak sesuai dengan gambar\")\n",
        "prediksi_komentar(\"lumayan, tapi pengemasannya biasa saja\")\n",
        "prediksi_komentar(\"barang rusak\")\n",
        "prediksi_komentar(\"Kasirnya lama\")\n",
        "prediksi_komentar (\" barangnya jelek\")\n",
        "prediksi_komentar (\"Pengirimannya lama banget anjerrrr\")\n",
        "prediksi_komentar (\"kasirnya ga ramah\")\n",
        "prediksi_komentar (\"Antrean kasir di Walmart terlalu panjang dan lambat.\")\n",
        "prediksi_komentar (\"asli disituh mahal bangett\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9174a9-a2d0-4d71-9bd0-c899937fcfdb",
        "id": "ql8pE9JkijyG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ—£ï¸ Input: pengiriman sangat cepat dan pelayanan ramah sekali\n",
            "ğŸ§¹ Cleaned: pengiriman cepat pelayanan ramah\n",
            "ğŸ“Š Prediksi Sentimen: positif\n",
            "\n",
            "ğŸ—£ï¸ Input: barang rusak, tidak sesuai dengan gambar\n",
            "ğŸ§¹ Cleaned: barang rusak tidak sesuai gambar\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n",
            "\n",
            "ğŸ—£ï¸ Input: lumayan, tapi pengemasannya biasa saja\n",
            "ğŸ§¹ Cleaned: lumayan pengemasannya biasa\n",
            "ğŸ“Š Prediksi Sentimen: netral\n",
            "\n",
            "ğŸ—£ï¸ Input: barang rusak\n",
            "ğŸ§¹ Cleaned: barang rusak\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n",
            "\n",
            "ğŸ—£ï¸ Input: Kasirnya lama\n",
            "ğŸ§¹ Cleaned: kasirnya lama\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n",
            "\n",
            "ğŸ—£ï¸ Input:  barangnya jelek\n",
            "ğŸ§¹ Cleaned: barangnya jelek\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n",
            "\n",
            "ğŸ—£ï¸ Input: Pengirimannya lama banget anjerrrr\n",
            "ğŸ§¹ Cleaned: pengirimannya lama banget anjer\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n",
            "\n",
            "ğŸ—£ï¸ Input: kasirnya ga ramah\n",
            "ğŸ§¹ Cleaned: kasirnya ga ramah\n",
            "ğŸ“Š Prediksi Sentimen: positif\n",
            "\n",
            "ğŸ—£ï¸ Input: Antrean kasir di Walmart terlalu panjang dan lambat.\n",
            "ğŸ§¹ Cleaned: antrean kasir walmart lambat\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n",
            "\n",
            "ğŸ—£ï¸ Input: asli disituh mahal bangett\n",
            "ğŸ§¹ Cleaned: asli disituh mahal bangett\n",
            "ğŸ“Š Prediksi Sentimen: negatif\n"
          ]
        }
      ]
    }
  ]
}