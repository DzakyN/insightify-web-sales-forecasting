{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM1lO1dh82G9+m1Vam+0fCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DzakyN/insightify-web-sales-forecasting/blob/main/Model_TIKEXPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forecasting metode agregasi model LSTM"
      ],
      "metadata": {
        "id": "PfLV8IEtP5ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Attention, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "import holidays\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "# 1 Seed\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "# 2Ô∏è Load data\n",
        "file_path = r\"C:\\Users\\mdzak\\Downloads\\Data_Final_Setelah_Hapus_Store_8.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "# 3Ô∏è Group by date\n",
        "df_total = df.groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index().sort_values(\"Date\")\n",
        "\n",
        "# 4Ô∏è Baseline moving average\n",
        "window_baseline = 154\n",
        "df_total[\"baseline_ma30\"] = df_total[\"Weekly_Sales\"].rolling(window=window_baseline, min_periods=1).mean()\n",
        "\n",
        "\n",
        "# 5Ô∏è Tambahkan fitur musiman & lag\n",
        "df_total[\"dayofmonth\"] = df_total[\"Date\"].dt.day\n",
        "df_total[\"weekofyear\"] = df_total[\"Date\"].dt.isocalendar().week\n",
        "df_total[\"is_month_start\"] = df_total[\"Date\"].dt.is_month_start.astype(int)\n",
        "df_total[\"sin_week\"] = np.sin(2 * np.pi * df_total[\"weekofyear\"] / 52)\n",
        "df_total[\"cos_week\"] = np.cos(2 * np.pi * df_total[\"weekofyear\"] / 52)\n",
        "df_total[\"lag_1\"] = df_total[\"Weekly_Sales\"].shift(1)\n",
        "df_total[\"lag_52\"] = df_total[\"Weekly_Sales\"].shift(52)\n",
        "\n",
        "\n",
        "# 6Ô∏è Tambahkan exogenous features\n",
        "us_holidays = holidays.US()\n",
        "df_total[\"is_holiday_us\"] = df_total[\"Date\"].apply(lambda x: 1 if x in us_holidays else 0)\n",
        "df_total[\"is_weekend\"] = df_total[\"Date\"].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)\n",
        "df_total[\"month\"] = df_total[\"Date\"].dt.month\n",
        "df_total[\"quarter\"] = df_total[\"Date\"].dt.quarter\n",
        "df_total[\"rolling_mean_7\"] = df_total[\"Weekly_Sales\"].rolling(window=7, min_periods=1).mean()\n",
        "df_total[\"rolling_std_7\"] = df_total[\"Weekly_Sales\"].rolling(window=7, min_periods=1).std()\n",
        "\n",
        "\n",
        "# Drop NA\n",
        "df_total.dropna(inplace=True)\n",
        "df_total.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "# 7Ô∏è Target residual\n",
        "df_total[\"residual_target\"] = df_total[\"Weekly_Sales\"] - df_total[\"baseline_ma30\"]\n",
        "\n",
        "\n",
        "# 8Ô∏è Features\n",
        "features = [\"Weekly_Sales\", \"lag_1\", \"lag_52\", \"dayofmonth\", \"is_month_start\",\n",
        "            \"sin_week\", \"cos_week\", \"is_holiday_us\", \"is_weekend\",\n",
        "            \"month\", \"quarter\", \"rolling_mean_7\", \"rolling_std_7\"]\n",
        "\n",
        "\n",
        "# Sliding window\n",
        "window_size = 84\n",
        "forecast_horizon = 90\n",
        "X, y = [], []\n",
        "for i in range(window_size, len(df_total) - forecast_horizon + 1):\n",
        "    X_window = df_total[features].iloc[i-window_size:i].values\n",
        "    y_horizon = df_total[\"residual_target\"].iloc[i:i+forecast_horizon].values\n",
        "    X.append(X_window)\n",
        "    y.append(y_horizon)\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "# Split\n",
        "n = len(X)\n",
        "cutoff_train = int(n * 0.67)\n",
        "cutoff_valid = int(n * 0.75)\n",
        "X_train, y_train = X[:cutoff_train], y[:cutoff_train]\n",
        "X_valid, y_valid = X[cutoff_train:cutoff_valid], y[cutoff_train:cutoff_valid]\n",
        "X_test, y_test = X[cutoff_valid:], y[cutoff_valid:]\n",
        "\n",
        "\n",
        "# Scaling\n",
        "y_scaler = MinMaxScaler()\n",
        "y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "y_valid_scaled = y_scaler.transform(y_valid)\n",
        "y_test_scaled = y_scaler.transform(y_test)\n",
        "\n",
        "\n",
        "x_scaler = MinMaxScaler()\n",
        "X_train_scaled = x_scaler.fit_transform(X_train.reshape(-1, len(features))).reshape((-1, window_size, len(features)))\n",
        "X_valid_scaled = x_scaler.transform(X_valid.reshape(-1, len(features))).reshape((-1, window_size, len(features)))\n",
        "X_test_scaled = x_scaler.transform(X_test.reshape(-1, len(features))).reshape((-1, window_size, len(features)))\n",
        "\n",
        "\n",
        "# Model stacked LSTM + Attention\n",
        "def create_model(input_shape, units=128, dropout=0.2, lr=0.001):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(units, return_sequences=True, dropout=dropout)(inputs)\n",
        "    x = LSTM(units, return_sequences=True, dropout=dropout)(x)\n",
        "    attn = Attention()([x, x])\n",
        "    concat = Concatenate()([x, attn])\n",
        "    x = Dense(256, activation='relu')(concat[:, -1, :])\n",
        "    x = Dropout(dropout)(x)\n",
        "    out = Dense(forecast_horizon)(x)\n",
        "    model = Model(inputs, out)\n",
        "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"units\": [64, 128],\n",
        "    \"dropout\": [0.1, 0.2],\n",
        "    \"lr\": [0.001, 0.0005],\n",
        "    \"batch_size\": [16, 32]\n",
        "}\n",
        "param_combinations = list(product(param_grid[\"units\"], param_grid[\"dropout\"],\n",
        "                                  param_grid[\"lr\"], param_grid[\"batch_size\"]))\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "best_r2 = -np.inf\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "\n",
        "for units, dropout, lr, batch_size in param_combinations:\n",
        "    print(f\"üîç Training: units={units}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "    model = create_model((window_size, len(features)), units=units, dropout=dropout, lr=lr)\n",
        "    es = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    model.fit(X_train_scaled, y_train_scaled,\n",
        "               validation_data=(X_valid_scaled, y_valid_scaled),\n",
        "               epochs=50, batch_size=batch_size, verbose=0, callbacks=[es])\n",
        "    y_valid_pred_scaled = model.predict(X_valid_scaled, verbose=0)\n",
        "    y_valid_pred = y_scaler.inverse_transform(y_valid_pred_scaled)\n",
        "    y_valid_real = y_scaler.inverse_transform(y_valid_scaled)\n",
        "    baseline_valid = df_total[\"baseline_ma30\"].iloc[cutoff_train+window_size:cutoff_valid+window_size].values.reshape(-1,1).repeat(forecast_horizon, axis=1)\n",
        "    y_valid_pred_final = y_valid_pred + baseline_valid\n",
        "    y_valid_real_final = y_valid_real + baseline_valid\n",
        "    mape = mean_absolute_percentage_error(y_valid_real_final, y_valid_pred_final) * 100\n",
        "    mae = mean_absolute_error(y_valid_real_final, y_valid_pred_final)\n",
        "    rmse = np.sqrt(mean_squared_error(y_valid_real_final, y_valid_pred_final))\n",
        "    r2 = r2_score(y_valid_real_final.flatten(), y_valid_pred_final.flatten())\n",
        "    results.append({\n",
        "        \"units\": units,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAPE\": round(mape, 2),\n",
        "        \"MAE\": round(mae, 2),\n",
        "        \"RMSE\": round(rmse, 2),\n",
        "        \"R2\": round(r2, 4)\n",
        "    })\n",
        "    print(f\"‚û°Ô∏è Valid Metrics: MAPE={mape:.2f}%, MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.4f}\")\n",
        "    if r2 > best_r2:\n",
        "        best_r2 = r2\n",
        "        best_model = model\n",
        "        best_params = (units, dropout, lr, batch_size)\n",
        "\n",
        "\n",
        "# Log semua hasil\n",
        "print(\"\\nüìù All Hyperparameter Combinations and Validation Metrics:\")\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Best Hyperparameters (R¬≤ Based):\")\n",
        "print(f\"Units: {best_params[0]}, Dropout: {best_params[1]}, LR: {best_params[2]}, Batch: {best_params[3]}\")\n",
        "\n",
        "\n",
        "#Final Evaluation on Test\n",
        "y_test_pred_scaled = best_model.predict(X_test_scaled)\n",
        "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled)\n",
        "y_test_real = y_scaler.inverse_transform(y_test_scaled)\n",
        "baseline_test = df_total[\"baseline_ma30\"].iloc[-len(y_test):].values.reshape(-1, 1).repeat(forecast_horizon, axis=1)\n",
        "y_test_pred_final = y_test_pred + baseline_test\n",
        "y_test_real_final = y_test_real + baseline_test\n",
        "\n",
        "\n",
        "test_mape = mean_absolute_percentage_error(y_test_real_final, y_test_pred_final) * 100\n",
        "test_mae = mean_absolute_error(y_test_real_final, y_test_pred_final)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test_real_final, y_test_pred_final))\n",
        "test_r2 = r2_score(y_test_real_final.flatten(), y_test_pred_final.flatten())\n",
        "\n",
        "\n",
        "print(\"\\nüéØ Final Test Metrics (Best Model - R¬≤ Based):\")\n",
        "print(\"‚úÖ MAPE:\", round(test_mape, 2), \"%\")\n",
        "print(\"‚úÖ MAE :\", round(test_mae, 2))\n",
        "print(\"‚úÖ RMSE:\", round(test_rmse, 2))\n",
        "print(\"‚úÖ R2  :\", round(test_r2, 4))\n",
        "\n",
        "\n",
        "#Rolling Forecast ke depan 30 hari\n",
        "last_window_real = df_total[features].values[-window_size:]\n",
        "last_window_scaled = x_scaler.transform(last_window_real).reshape(1, window_size, len(features))\n",
        "forecast_scaled = best_model.predict(last_window_scaled)[0]\n",
        "forecast_residual = y_scaler.inverse_transform(forecast_scaled.reshape(1, -1)).flatten()\n",
        "baseline_future = df_total[\"baseline_ma30\"].iloc[-1]\n",
        "forecast_final = forecast_residual + baseline_future\n",
        "\n",
        "\n",
        "# Save & visualisasi\n",
        "last_date = df_total[\"Date\"].max()\n",
        "future_dates = [last_date + timedelta(days=i) for i in range(1, forecast_horizon + 1)]\n",
        "df_forecast = pd.DataFrame({\"Date\": future_dates, \"Forecast\": forecast_final})\n",
        "df_actual = df_total[[\"Date\", \"Weekly_Sales\"]].rename(columns={\"Weekly_Sales\": \"Actual\"})\n",
        "df_final = pd.concat([df_actual, df_forecast.rename(columns={\"Forecast\": \"Actual\"})], ignore_index=True)\n",
        "df_final.to_csv(\"Total_Sales_LSTM_Hybrid_USA_Forecast.csv\", index=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_actual[\"Date\"], df_actual[\"Actual\"], label=\"Actual\")\n",
        "plt.plot(df_forecast[\"Date\"], df_forecast[\"Forecast\"], label=\"Forecast\", linestyle=\"--\")\n",
        "plt.axvline(x=last_date, color='red', linestyle=':', label=\"Forecast Start\")\n",
        "plt.title(f\"Hybrid LSTM+Attention 30D Forecast (MAPE: {test_mape:.2f}%)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"üéâ Final hybrid residual forecast & hyperparameter tuning selesai!\")\n"
      ],
      "metadata": {
        "id": "MtHOyj0Upoap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Forecasting metode Granulalitas model Prophet (105 Kombinasi)"
      ],
      "metadata": {
        "id": "sqW7N47CP9-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv(r\"C:\\Semester 6\\Setsuna\\Data_Aktual_Tanpa_Forecast.csv\")\n",
        "\n",
        "df['ds'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df = df.rename(columns={'Daily_Sales': 'y'})\n",
        "\n",
        "df = df[['ds', 'Store', 'Dept', 'y']].sort_values('ds')\n",
        "\n",
        "stores = df['Store'].unique()\n",
        "depts = df['Dept'].unique()\n",
        "\n",
        "\n",
        "# Grid search parameter\n",
        "param_grid = {\n",
        "    'changepoint_prior_scale': [0.001, 0.01, 0.1,0.2, 0.5, 1.0,10.0],\n",
        "    'seasonality_prior_scale': [0.001,0.01, 0.1, 1.0,0.2,0.5, 10.0]\n",
        "}\n",
        "\n",
        "# Hasil akhir\n",
        "evaluasi_list = []\n",
        "forecast_list = []\n",
        "\n",
        "\n",
        "for store in stores:\n",
        "    for dept in depts:\n",
        "        subset = df[(df['Store'] == store) & (df['Dept'] == dept)].copy()\n",
        "        if len(subset) < 100:\n",
        "            continue\n",
        "        # Split train-valid-test\n",
        "        n = len(subset)\n",
        "        cutoff_train = int(n * 0.7)\n",
        "        cutoff_valid = int(n * 0.87)\n",
        "\n",
        "\n",
        "        df_train = subset.iloc[:cutoff_train].copy()\n",
        "        df_valid = subset.iloc[cutoff_train:cutoff_valid].copy()\n",
        "        df_test = subset.iloc[cutoff_valid:].copy()\n",
        "\n",
        "\n",
        "        # Tuning\n",
        "        tuning_results = []\n",
        "        for cps in param_grid['changepoint_prior_scale']:\n",
        "            for sps in param_grid['seasonality_prior_scale']:\n",
        "                try:\n",
        "                    model = Prophet(\n",
        "                        daily_seasonality=True,\n",
        "                        weekly_seasonality=True,\n",
        "                        yearly_seasonality=True,\n",
        "                        changepoint_prior_scale=cps,\n",
        "                        seasonality_prior_scale=sps\n",
        "                    )\n",
        "                    model.fit(df_train)\n",
        "                    forecast_valid = model.predict(df_valid[['ds']])\n",
        "                    y_true = df_valid['y'].values\n",
        "                    y_pred = forecast_valid['yhat'].values\n",
        "                    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-5))) * 100\n",
        "                    tuning_results.append({\n",
        "                        'cps': cps,\n",
        "                        'sps': sps,\n",
        "                        'mape': mape\n",
        "                    })\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not tuning_results:\n",
        "            continue\n",
        "\n",
        "        best_param = sorted(tuning_results, key=lambda x: x['mape'])[0]\n",
        "        best_cps = best_param['cps']\n",
        "        best_sps = best_param['sps']\n",
        "\n",
        "\n",
        "        # Rolling Forecast pada Test Set\n",
        "        rolling_train = pd.concat([df_train, df_valid])\n",
        "        y_true_all = []\n",
        "        y_pred_all = []\n",
        "\n",
        "        for i in range(len(df_test)):\n",
        "            model = Prophet(\n",
        "                daily_seasonality=True,\n",
        "                weekly_seasonality=True,\n",
        "                yearly_seasonality=True,\n",
        "                changepoint_prior_scale=best_cps,\n",
        "                seasonality_prior_scale=best_sps\n",
        "            )\n",
        "            model.fit(rolling_train)\n",
        "            future = df_test[['ds']].iloc[i:i+1]\n",
        "            forecast = model.predict(future)\n",
        "\n",
        "\n",
        "            y_true_all.append(df_test['y'].iloc[i])\n",
        "            y_pred_all.append(forecast['yhat'].values[0])\n",
        "            rolling_train = pd.concat([rolling_train, df_test.iloc[i:i+1]], ignore_index=True)\n",
        "\n",
        "        # Evaluasi\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
        "        mae = mean_absolute_error(y_true_all, y_pred_all)\n",
        "        mape = np.mean(np.abs((np.array(y_true_all) - np.array(y_pred_all)) / np.maximum(np.abs(y_true_all), 1e-5))) * 100\n",
        "\n",
        "\n",
        "        evaluasi_list.append({\n",
        "            'Store': store,\n",
        "            'Dept': dept,\n",
        "            'Best CPS': best_cps,\n",
        "            'Best SPS': best_sps,\n",
        "            'RMSE': round(rmse, 3),\n",
        "            'MAE': round(mae, 3),\n",
        "            'MAPE': round(mape, 6)\n",
        "        })\n",
        "\n",
        "\n",
        "        # atur forecast\n",
        "        full_df = pd.concat([df_train, df_valid, df_test])\n",
        "        model_final = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=True,\n",
        "            changepoint_prior_scale=best_cps,\n",
        "            seasonality_prior_scale=best_sps\n",
        "        )\n",
        "        model_final.fit(full_df)\n",
        "        future = model_final.make_future_dataframe(periods=90)\n",
        "        forecast = model_final.predict(future)\n",
        "        last_date = full_df['ds'].max()\n",
        "        forecast_future = forecast[forecast['ds'] > last_date].copy()\n",
        "        forecast_future['Store'] = store\n",
        "        forecast_future['Dept'] = dept\n",
        "        forecast_list.append(forecast_future[['ds', 'Store', 'Dept', 'yhat']])\n",
        "\n",
        "\n",
        "# Simpan Evaluasi dan Forecast\n",
        "df_evaluasi = pd.DataFrame(evaluasi_list)\n",
        "df_evaluasi.to_csv(\"evaluasi_model2.csv\", index=False)\n",
        "\n",
        "\n",
        "df_forecast_all = pd.concat(forecast_list, ignore_index=True)\n",
        "df_forecast_all = df_forecast_all.rename(columns={'ds': 'Date', 'yhat': 'Daily_Sales'})\n",
        "df_forecast_all.to_csv(\"Forecast Final.csv\", index=False)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Selesai: evaluasi_model.csv & forecast_90hari.csv berhasil dibuat.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kJ3b6DnJpc7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentimen Analisis"
      ],
      "metadata": {
        "id": "L9rXDcaWZC_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk==3.8.1 -q\n",
        "!pip install -U imbalanced-learn -q\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree('/content/nltk_data', ignore_errors=True)\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import joblib  # untuk menyimpan model/vectorizer\n",
        "\n",
        "==\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv(\"/content/Data Sentimen.csv\")\n",
        "\n",
        "# 2. Stopwords default dari NLTK (Bahasa Indonesia)\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# 3. Kata-kata penting yang TIDAK BOLEH DIHAPUS karena punya makna sentimen/konteks\n",
        "kata_penting = {\n",
        "    # Penyangkalan / penekanan\n",
        "    'tidak', 'kurang', 'belum', 'hanya', 'seharusnya',\n",
        "\n",
        "    # Kata-kata temporal / kecepatan / kualitas layanan\n",
        "    'lama', 'cepat', 'lambat', 'terlambat', 'lelet',\n",
        "\n",
        "    # Kata netral / ambiguitas\n",
        "    'biasa', 'lumayan', 'cukup', 'standar','saja'\n",
        "\n",
        "    # Kata negatif umum\n",
        "    'buruk', 'jelek', 'rusak', 'cacat', 'telat',\n",
        "\n",
        "    # Kata positif umum\n",
        "    'bagus', 'baik', 'puas', 'mantap', 'ramah',\n",
        "\n",
        "    # Kata ekspresi\n",
        "    'wah', 'wow', 'parah', 'sumpah', 'love', 'suka', 'senang',\n",
        "\n",
        "    # Kata soal pelayanan / barang\n",
        "    'kasir', 'produk', 'barang', 'layanan', 'pengiriman', 'kurir'\n",
        "}\n",
        "\n",
        "# 4. Hapus kata penting dari stopwords agar tidak terhapus saat filtering\n",
        "stop_words -= kata_penting\n",
        "\n",
        "# 5. Fungsi preprocessing\n",
        "def preprocess_nltk(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)               # hilangkan huruf berulang, misal baaagus ‚Üí bagus\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)                # hilangkan karakter non-alfabet\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "# 6. Terapkan preprocessing\n",
        "df['clean_text'] = df['review_text'].astype(str).apply(preprocess_nltk)\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=7000,\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "X = tfidf.fit_transform(df['clean_text'])\n",
        "y = df['label_sentimen']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nLaporan klasifikasi:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "joblib.dump(model, 'model_rf.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uoyy_ze_SKpT",
        "outputId": "4e07ff3f-3305-4434-f3a8-35c34624771c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.9325581395348838\n",
            "\n",
            "Laporan klasifikasi:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.90      0.91      0.91       956\n",
            "      netral       0.98      0.93      0.96       722\n",
            "     positif       0.93      0.95      0.94      1332\n",
            "\n",
            "    accuracy                           0.93      3010\n",
            "   macro avg       0.94      0.93      0.93      3010\n",
            "weighted avg       0.93      0.93      0.93      3010\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üé§ UJI INPUT MANUAL\n",
        "# ================================\n",
        "\n",
        "def prediksi_komentar(text_input):\n",
        "    # Preprocessing\n",
        "    clean = preprocess_nltk(text_input)\n",
        "\n",
        "    # TF-IDF transform (jangan .fit_transform lagi, cukup .transform!)\n",
        "    vectorized = tfidf.transform([clean])\n",
        "\n",
        "    # Prediksi\n",
        "    hasil = model.predict(vectorized)[0]\n",
        "\n",
        "    print(f\"\\nüó£Ô∏è Input: {text_input}\")\n",
        "    print(f\"üßπ Cleaned: {clean}\")\n",
        "    print(f\"üìä Prediksi Sentimen: {hasil}\")\n",
        "\n",
        "# Contoh uji coba:\n",
        "prediksi_komentar(\"pengiriman sangat cepat dan pelayanan ramah sekali\")\n",
        "prediksi_komentar(\"barang rusak, tidak sesuai dengan gambar\")\n",
        "prediksi_komentar(\"lumayan, tapi pengemasannya biasa saja\")\n",
        "prediksi_komentar(\"barang rusak\")\n",
        "prediksi_komentar(\"Kasirnya lama\")\n",
        "prediksi_komentar (\" barangnya jelek\")\n",
        "prediksi_komentar (\"Pengirimannya lama banget anjerrrr\")\n",
        "prediksi_komentar (\"kasirnya ga ramah\")\n",
        "prediksi_komentar (\"Antrean kasir di Walmart terlalu panjang dan lambat.\")\n",
        "prediksi_komentar (\"asli disituh mahal bangett\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9174a9-a2d0-4d71-9bd0-c899937fcfdb",
        "id": "ql8pE9JkijyG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üó£Ô∏è Input: pengiriman sangat cepat dan pelayanan ramah sekali\n",
            "üßπ Cleaned: pengiriman cepat pelayanan ramah\n",
            "üìä Prediksi Sentimen: positif\n",
            "\n",
            "üó£Ô∏è Input: barang rusak, tidak sesuai dengan gambar\n",
            "üßπ Cleaned: barang rusak tidak sesuai gambar\n",
            "üìä Prediksi Sentimen: negatif\n",
            "\n",
            "üó£Ô∏è Input: lumayan, tapi pengemasannya biasa saja\n",
            "üßπ Cleaned: lumayan pengemasannya biasa\n",
            "üìä Prediksi Sentimen: netral\n",
            "\n",
            "üó£Ô∏è Input: barang rusak\n",
            "üßπ Cleaned: barang rusak\n",
            "üìä Prediksi Sentimen: negatif\n",
            "\n",
            "üó£Ô∏è Input: Kasirnya lama\n",
            "üßπ Cleaned: kasirnya lama\n",
            "üìä Prediksi Sentimen: negatif\n",
            "\n",
            "üó£Ô∏è Input:  barangnya jelek\n",
            "üßπ Cleaned: barangnya jelek\n",
            "üìä Prediksi Sentimen: negatif\n",
            "\n",
            "üó£Ô∏è Input: Pengirimannya lama banget anjerrrr\n",
            "üßπ Cleaned: pengirimannya lama banget anjer\n",
            "üìä Prediksi Sentimen: negatif\n",
            "\n",
            "üó£Ô∏è Input: kasirnya ga ramah\n",
            "üßπ Cleaned: kasirnya ga ramah\n",
            "üìä Prediksi Sentimen: positif\n",
            "\n",
            "üó£Ô∏è Input: Antrean kasir di Walmart terlalu panjang dan lambat.\n",
            "üßπ Cleaned: antrean kasir walmart lambat\n",
            "üìä Prediksi Sentimen: negatif\n",
            "\n",
            "üó£Ô∏è Input: asli disituh mahal bangett\n",
            "üßπ Cleaned: asli disituh mahal bangett\n",
            "üìä Prediksi Sentimen: negatif\n"
          ]
        }
      ]
    }
  ]
}